{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Create Word2Vec Embeddings"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21058,"status":"ok","timestamp":1721235979484,"user":{"displayName":"Kylie Trousil","userId":"16813790667997979199"},"user_tz":420},"id":"w0Eq_UzLrfV3","outputId":"2fff76ee-de4d-42cd-8f53-5bd3e4a7543f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["###\n","# imports\n","###\n","\n","# for running in Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVp2hDYgQOey","outputId":"c919b41e-faee-4ad2-92bf-e2c48c7c7075"},"outputs":[{"name":"stdout","output_type":"stream","text":["subfolder_path: /content/drive/MyDrive/REU/ResearchProject/v077_clean/Vobfus\n"]}],"source":["def read_file_as_one_document(filename):\n","    with open(filename, 'r') as f:\n","        text = f.read()\n","        tokens = word_tokenize(text)\n","        return tokens\n","\n","def get_documents_from_subfolders(base_path, subfolder, num_files=2, subset = False):\n","    documents = []\n","    # labels = []\n","    subfolder_path = os.path.join(base_path, subfolder)\n","    print(f\"subfolder_path: {subfolder_path}\")\n","\n","    # if subset:\n","    #   files = sorted(os.listdir(subfolder_path))[:num_files]  # Get the first x files from each subfolder\n","    # else:\n","    #   files = sorted(os.listdir(subfolder_path))              # Get all the files\n","    files = os.listdir(subfolder_path)\n","    for filename in files:\n","        if filename.endswith('.txt'):\n","            file_path = os.path.join(subfolder_path, filename)\n","            documents.append(read_file_as_one_document(file_path))\n","            # labels.append(subfolder)  # Add the label for the subfolder\n","    return documents\n","\n","# document vector\n","def document_vector(word2vec_model, doc_tokens):\n","    doc_tokens = [token for token in doc_tokens if token in word2vec_model.wv.key_to_index]\n","    if not doc_tokens:\n","        return np.zeros(word2vec_model.vector_size)\n","    return np.mean(word2vec_model.wv[doc_tokens], axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Conversion Code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_path = '/content/drive/MyDrive/REU/ResearchProject/v077_clean/'\n","#### from maleware2\n","# subfolders = ['winwebsec', 'zeroaccess'] #'zbot',\n","#### from v077\n","subfolders = ['Vobfus', 'Diplugem', 'Obfuscator', 'Vundo', 'VBInject',\n","              'Delf', 'Beebone', 'Enterak.A','OnLineGames',\n","              'Startpage', 'Allaple.A', 'Injector', 'Systex.A', 'Expiro.BK',\n","              'FakeRean', 'Small', 'Toga!rfn', 'Lamechi.B', 'CeeInject',\n","              'Renos', 'Hotbar', 'DelfInject']\n","vector_size = 104\n","\n","\n","for family in subfolders:\n","  # Get the documents from each subfolder\n","  documents = get_documents_from_subfolders(base_path, subfolder = family)\n","\n","  print(f\"family: {family}\")\n","  print(f\"documents: {len(documents)}\")\n","  print(\"------------------------------------------------------\")\n","\n","  #Word2Vec model\n","  word2vec_model = Word2Vec(sentences=documents, vector_size=vector_size,  window=10, min_count=0, workers=4, seed=42)\n","\n","  #document embedding\n","  doc_vectors = [document_vector(word2vec_model, doc) for doc in documents]\n","  doc_vectors_df = pd.DataFrame(doc_vectors)\n","\n","  #normalize to [0,1]\n","  max_val = doc_vectors_df.to_numpy().max()\n","  min_val = doc_vectors_df.to_numpy().min()\n","  range_vals = max_val - min_val\n","\n","  for i in range(vector_size):\n","    doc_vectors_df[i] = (doc_vectors_df[i] - min_val) / range_vals\n","\n","  #save csv\n","  new_filename = str(f\"data/embeddings/top25_{vector_size}/{family}.csv\")\n","  doc_vectors_df.to_csv(new_filename, header=False, index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"OVrk7MCjkOT1"},"source":["#"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
